[{"path":"https://docs.ropensci.org/robotstxt/articles/using_robotstxt.html","id":"description","dir":"Articles","previous_headings":"","what":"Description","title":"Using Robotstxt","text":"package provides simple ‘robotstxt’ class accompanying methods parse check ‘robots.txt’ files. Data fields provided data frames vectors. Permissions can checked providing path character vectors optional bot names.","code":""},{"path":"https://docs.ropensci.org/robotstxt/articles/using_robotstxt.html","id":"robots-txt-files","dir":"Articles","previous_headings":"","what":"Robots.txt files","title":"Using Robotstxt","text":"Robots.txt files way kindly ask webbots, spiders, crawlers, wanderers like access access certain parts webpage. de facto ‘standard’ never made beyond informal “Network Working Group INTERNET DRAFT”. Nonetheless, use robots.txt files widespread (e.g. https://en.wikipedia.org/robots.txt, https://www.google.com/robots.txt) bots Google, Yahoo like adhere rules defined robots.txt files - although, interpretation rules might differ (e.g. rules googlebot). name files already suggests robots.txt files plain text always found root domain. syntax files essence follows fieldname: value scheme optional preceding user-agent: ... lines indicate scope following rule block. Blocks separated blank lines omission user-agent field (directly corresponds HTTP user-agent field) seen referring bots. # serves comment lines parts lines. Everything # end line regarded comment. Possible field names : user-agent, disallow, allow, crawl-delay, sitemap, host. Let us example file get idea robots.txt file might look like. file starts comment line followed line disallowing access content – everything contained root (“/”) – bots. next block concerns GoodBot NiceBot. two get previous permissions lifted disallowed nothing. third block PrettyBot. PrettyBot likes shiny stuff therefor gets special permission everything contained “/shinystuff/” folder restrictions still hold. last block bots asked pause least 5 seconds two visits. information look : http://www.robotstxt.org/norobots-rfc.txt, robots.txt file ‘standard’ described formally. Valuable introductions can found http://www.robotstxt.org/robotstxt.html well https://en.wikipedia.org/wiki/Robots_exclusion_standard - cause.","code":"# this is a comment  # a made up example of an robots.txt file  Disallow: /  User-agent: GoodBot # another comment User-agent: NiceBot Disallow:   User-agent: PrettyBot Allow: /shinystuff/  Crawl-Delay: 5"},{"path":"https://docs.ropensci.org/robotstxt/articles/using_robotstxt.html","id":"fast-food-usage-for-the-uninterested","dir":"Articles","previous_headings":"","what":"Fast food usage for the uninterested","title":"Using Robotstxt","text":"","code":"library(robotstxt) paths_allowed(\"http://google.com/\") ## [1] TRUE paths_allowed(\"http://google.com/search\") ## [1] FALSE"},{"path":"https://docs.ropensci.org/robotstxt/articles/using_robotstxt.html","id":"example-usage","dir":"Articles","previous_headings":"","what":"Example Usage","title":"Using Robotstxt","text":"First, let us load package. addition load dplyr package able use magrittr pipe operator %>% easy read remember data manipulation functions.","code":"library(robotstxt) library(dplyr)"},{"path":"https://docs.ropensci.org/robotstxt/articles/using_robotstxt.html","id":"object-oriented-style","dir":"Articles","previous_headings":"Example Usage","what":"object oriented style","title":"Using Robotstxt","text":"first step create instance robotstxt class provided package. instance initiated via providing either domain actual text robots.txt file. domain provided, robots.txt file downloaded automatically. look ?robotstxt descriptions data fields methods well parameters. rtxt class robotstxt. Printing object lets us glance data fields methods rtxt - access text well common fields. Non-standard fields collected . Checking permissions works via rtxt’s check method providing one paths. bot name provided \"*\" - meaning bot - assumed.","code":"rtxt <- robotstxt(domain=\"wikipedia.org\") class(rtxt) ## [1] \"robotstxt\" rtxt ## $text ## [1] \"#\\n# robots.txt for http://www.wikipedia.org/ and friends\\n#\\n# Please note: There are a lot of pages on this site, and there are\\n# some misbehaved spiders out there that go _way_ too fast. If you're\\n# irresponsible, your access to the site may be blocked.\\n#\\n\\n# advertising-related bots:\\nUser-agent: Mediapartners-Google*\\n\\n[... 653 lines omitted ...]\" ##  ## $domain ## [1] \"wikipedia.org\" ##  ## $robexclobj ## <Robots Exclusion Protocol Object> ## $bots ## [1] \"Mediapartners-Google*\"       \"IsraBot\"                     ## [3] \"Orthogaffe\"                  \"UbiCrawler\"                  ## [5] \"DOC\"                         \"Zao\"                         ## [7] \"\"                            \"[...  28 items omitted ...]\" ##  ## $comments ##   line                                                               comment ## 1    1                                                                     # ## 2    2                # robots.txt for http://www.wikipedia.org/ and friends ## 3    3                                                                     # ## 4    4   # Please note: There are a lot of pages on this site, and there are ## 5    5 # some misbehaved spiders out there that go _way_ too fast. If you're ## 6    6              # irresponsible, your access to the site may be blocked. ## 7                                                                            ## 8                                               [...  173 items omitted ...] ##  ## $permissions ##                          field             useragent value ## 1                     Disallow Mediapartners-Google*     / ## 2                     Disallow               IsraBot       ## 3                     Disallow            Orthogaffe       ## 4                     Disallow            UbiCrawler     / ## 5                     Disallow                   DOC     / ## 6                     Disallow                   Zao     / ## 7                                                          ## 8 [...  370 items omitted ...]                             ##  ## $crawl_delay ## [1] field     useragent value     ## <0 rows> (or 0-length row.names) ##  ## $host ## [1] field     useragent value     ## <0 rows> (or 0-length row.names) ##  ## $sitemap ## [1] field     useragent value     ## <0 rows> (or 0-length row.names) ##  ## $other ## [1] field     useragent value     ## <0 rows> (or 0-length row.names) ##  ## $check ## function (paths = \"/\", bot = \"*\")  ## { ##     spiderbar::can_fetch(obj = self$robexclobj, path = paths,  ##         user_agent = bot) ## } ## <bytecode: 0x55be51942c60> ## <environment: 0x55be51943868> ##  ## attr(,\"class\") ## [1] \"robotstxt\" # checking for access permissions rtxt$check(paths = c(\"/\",\"api/\"), bot = \"*\") ## [1]  TRUE FALSE rtxt$check(paths = c(\"/\",\"api/\"), bot = \"Orthogaffe\") ## [1] TRUE TRUE rtxt$check(paths = c(\"/\",\"api/\"), bot = \"Mediapartners-Google*  \") ## [1]  TRUE FALSE"},{"path":"https://docs.ropensci.org/robotstxt/articles/using_robotstxt.html","id":"functional-style","dir":"Articles","previous_headings":"Example Usage","what":"functional style","title":"Using Robotstxt","text":"working robotstxt class recommended checking can done functions well. following (1) download robots.txt file; (2) parse (3) check permissions.","code":"r_text        <- get_robotstxt(\"nytimes.com\") r_parsed <- parse_robotstxt(r_text) r_parsed ## $useragents ## [1] \"*\"                    \"Mediapartners-Google\" \"AdsBot-Google\"        ## [4] \"adidxbot\"             ##  ## $comments ## [1] line    comment ## <0 rows> (or 0-length row.names) ##  ## $permissions ##       field            useragent                                 value ## 1     Allow                    *                          /ads/public/ ## 2     Allow                    *             /svc/news/v3/all/pshb.rss ## 3  Disallow                    *                                 /ads/ ## 4  Disallow                    *                             /adx/bin/ ## 5  Disallow                    *                            /archives/ ## 6  Disallow                    *                                /auth/ ## 7  Disallow                    *                                /cnet/ ## 8  Disallow                    *                             /college/ ## 9  Disallow                    *                            /external/ ## 10 Disallow                    *                      /financialtimes/ ## 11 Disallow                    *                                 /idg/ ## 12 Disallow                    *                             /indexes/ ## 13 Disallow                    *                             /library/ ## 14 Disallow                    *                    /nytimes-partners/ ## 15 Disallow                    * /packages/flash/multimedia/TEMPLATES/ ## 16 Disallow                    *                       /pages/college/ ## 17 Disallow                    *                         /paidcontent/ ## 18 Disallow                    *                            /partners/ ## 19 Disallow                    *                  /restaurants/search* ## 20 Disallow                    *                             /reuters/ ## 21 Disallow                    *                             /register ## 22 Disallow                    *                           /thestreet/ ## 23 Disallow                    *                                  /svc ## 24 Disallow                    *                     /video/embedded/* ## 25 Disallow                    *                        /web-services/ ## 26 Disallow                    *               /gst/travel/travsearch* ## 27 Disallow Mediapartners-Google                  /restaurants/search* ## 28 Disallow        AdsBot-Google                  /restaurants/search* ## 29 Disallow             adidxbot                  /restaurants/search* ##  ## $crawl_delay ## [1] field     useragent value     ## <0 rows> (or 0-length row.names) ##  ## $sitemap ##     field useragent ## 1 Sitemap         * ## 2 Sitemap         * ## 3 Sitemap         * ##                                                                    value ## 1 http://spiderbites.nytimes.com/sitemaps/www.nytimes.com/sitemap.xml.gz ## 2            http://www.nytimes.com/sitemaps/sitemap_news/sitemap.xml.gz ## 3   http://spiderbites.nytimes.com/sitemaps/sitemap_video/sitemap.xml.gz ##  ## $host ## [1] field     useragent value     ## <0 rows> (or 0-length row.names) ##  ## $other ## [1] field     useragent value     ## <0 rows> (or 0-length row.names) paths_allowed(   paths  = c(\"images/\",\"/search\"),    domain = c(\"wikipedia.org\", \"google.com\"),   bot    = \"Orthogaffe\" ) ##   wikipedia.org                        google.com ## [1]  TRUE FALSE"},{"path":"https://docs.ropensci.org/robotstxt/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Pedro Baltazar. Author, maintainer. Peter Meissner. Author. Kun Ren. Author, copyright holder.            Author copyright holder list_merge.R. Oliver Keys. Contributor.            original release code review Rich Fitz John. Contributor.            original release code review","code":""},{"path":"https://docs.ropensci.org/robotstxt/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Baltazar P, Meissner P, Ren K (2024). robotstxt: 'robots.txt' Parser 'Webbot'/'Spider'/'Crawler' Permissions Checker. R package version 0.7.13, https://github.com/ropensci/robotstxt, https://docs.ropensci.org/robotstxt/.","code":"@Manual{,   title = {robotstxt: A 'robots.txt' Parser and 'Webbot'/'Spider'/'Crawler' Permissions Checker},   author = {Pedro Baltazar and Peter Meissner and Kun Ren},   year = {2024},   note = {R package version 0.7.13, https://github.com/ropensci/robotstxt},   url = {https://docs.ropensci.org/robotstxt/}, }"},{"path":"https://docs.ropensci.org/robotstxt/index.html","id":"a-robotstxt-parser-and-webbotspidercrawler-permissions-checker","dir":"","previous_headings":"","what":"A ‘robots.txt’ Parser and ‘Webbot’/‘Spider’/‘Crawler’ Permissions Checker","title":"A robots.txt Parser and Webbot/Spider/Crawler Permissions Checker","text":"Status lines R code: 1007, lines test code: 1758  Development version 0.7.13 - 2020-08-19 / 20:39:24 Description Provides functions download parse ‘robots.txt’ files. Ultimately package makes easy check bots (spiders, crawler, scrapers, …) allowed access specific resources domain. License MIT + file LICENSE Peter Meissner [aut, cre], Kun Ren [aut, cph] (Author copyright holder list_merge.R.), Oliver Keys [ctb] (original release code review), Rich Fitz John [ctb] (original release code review) Citation BibTex citing Contribution - AKA -Think-Twice--Nice-Rule Please note project released Contributor Code Conduct. participating project agree abide terms: contributors maintainers project, pledge respect people contribute reporting issues, posting feature requests, updating documentation, submitting pull requests patches, activities. committed making participation project harassment-free experience everyone, regardless level experience, gender, gender identity expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, religion. Examples unacceptable behavior participants include use sexual language imagery, derogatory comments personal attacks, trolling, public private harassment, insults, unprofessional conduct. Project maintainers right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct. Project maintainers follow Code Conduct may removed project team. Instances abusive, harassing, otherwise unacceptable behavior may reported opening issue contacting one project maintainers. Code Conduct adapted Contributor Covenant (https://www.contributor-covenant.org/), version 1.0.0, available https://www.contributor-covenant.org/version/1/0/0/code--conduct/","code":"citation(\"robotstxt\") toBibtex(citation(\"robotstxt\"))"},{"path":"https://docs.ropensci.org/robotstxt/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"A robots.txt Parser and Webbot/Spider/Crawler Permissions Checker","text":"Installation start - stable version Installation start - development version","code":"install.packages(\"robotstxt\") library(robotstxt) devtools::install_github(\"ropensci/robotstxt\") library(robotstxt)"},{"path":"https://docs.ropensci.org/robotstxt/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"A robots.txt Parser and Webbot/Spider/Crawler Permissions Checker","text":"Robotstxt class documentation Simple path access right checking (functional way) … … (object oriented way) …","code":"?robotstxt library(robotstxt) options(robotstxt_warn = FALSE)   paths_allowed(   paths  = c(\"/api/rest_v1/?doc\", \"/w/\"),    domain = \"wikipedia.org\",    bot    = \"*\" ) ##  wikipedia.org ## [1]  TRUE FALSE  paths_allowed(   paths = c(     \"https://wikipedia.org/api/rest_v1/?doc\",      \"https://wikipedia.org/w/\"   ) ) ##  wikipedia.org                       wikipedia.org ## [1]  TRUE FALSE library(robotstxt) options(robotstxt_warn = FALSE)  rtxt <-    robotstxt(domain = \"wikipedia.org\")  rtxt$check(   paths = c(\"/api/rest_v1/?doc\", \"/w/\"),    bot   = \"*\" ) ## [1]  TRUE FALSE"},{"path":"https://docs.ropensci.org/robotstxt/index.html","id":"retrieval","dir":"","previous_headings":"Usage","what":"Retrieval","title":"A robots.txt Parser and Webbot/Spider/Crawler Permissions Checker","text":"Retrieving robots.txt file domain:","code":"# retrieval rt <-    get_robotstxt(\"https://petermeissner.de\")  # printing rt ## [robots.txt] ## -------------------------------------- ##  ## # just do it - punk"},{"path":"https://docs.ropensci.org/robotstxt/index.html","id":"interpretation","dir":"","previous_headings":"Usage","what":"Interpretation","title":"A robots.txt Parser and Webbot/Spider/Crawler Permissions Checker","text":"Checking whether one supposadly allowed access resource web server - unfortunately - just matter downloading parsing simple robots.txt file. First official specification robots.txt files every robots.txt file written every robots.txt file read used interpretation. time common understanding things supposed work things get complicated edges. interpretation problems: finding robots.txt file server (e.g. HTTP status code 404) implies everything allowed subdomains robots.txt file assumed everything allowed redirects involving protocol changes - e.g. upgrading http https - followed considered domain subdomain change - whatever found end redirect considered robots.txt file original domain redirects subdomain www doamin considered domain change - whatever found end redirect considered robots.txt file subdomain originally requested","code":""},{"path":"https://docs.ropensci.org/robotstxt/index.html","id":"event-handling","dir":"","previous_headings":"Usage","what":"Event Handling","title":"A robots.txt Parser and Webbot/Spider/Crawler Permissions Checker","text":"interpretation robots.txt rules just depends rules specified within file, package implements event handler system allows interpret re-interpret events rules. hood rt_request_handler() function called within get_robotstxt(). function takes {httr} request-response object set event handlers. Processing request handlers checks various events states around getting file reading content. event/state happened event handlers passed request_handler_handler() along problem resolution collecting robots.txt file transformations: rule priorities decide rules applied given current state priority rules specify signals emitted (e.g. error, message, warning) often rules imply overwriting raw content suitable interpretation given circumstances file () retrieved Event handler rules can either consist 4 items can functions - former usual case used throughout package . Functions like paths_allowed() parameters allow passing along handler rules handler functions. Handler rules lists following items: over_write_file_with: rule triggered higher priority rules applied beforehand (.e. new priority higher value old priority) robots.txt file retrieved overwritten character vector signal: might \"message\", \"warning\", \"error\" use signal function signal event/state just handled. Signaling warning message might suppressed setting function paramter warn = FALSE. cache package allowed cache results retrieval priority priority rule specified numeric value, rules higher priority allowed overwrite robots.txt file content changed rules lower priority package knows following rules following defaults: on_server_error : given server error - server unable serve file - assume something terrible wrong forbid paths time cache result might get updated file later on_client_error : client errors encompass HTTP status 4xx status codes except 404 handled directly despite fact lot codes might indicate client take action (authentication, billing, … see: https://de.wikipedia.org/wiki/HTTP-Statuscode) case retrieving robots.txt simple GET request things just work client error treated file available thus scraping generally allowed on_not_found : HTTP status code 404 handler treated ways client errors: file available thus scraping generally allowed on_redirect : redirects ok - often redirects redirect HTTP schema HTTPS - robotstxt use whatever content redirected on_domain_change : domain changes handled robots.txt file exist thus scraping generally allowed on_file_type_mismatch : {robotstxt} gets content content type text probably robotstxt file, situation handled file provided thus scraping generally allowed on_suspect_content : {robotstxt} parse probably robotstxt file, situation handled file provided thus scraping generally allowed","code":"on_server_error_default ## $over_write_file_with ## [1] \"User-agent: *\\nDisallow: /\" ##  ## $signal ## [1] \"error\" ##  ## $cache ## [1] FALSE ##  ## $priority ## [1] 20 on_client_error_default ## $over_write_file_with ## [1] \"User-agent: *\\nAllow: /\" ##  ## $signal ## [1] \"warning\" ##  ## $cache ## [1] TRUE ##  ## $priority ## [1] 19 on_not_found_default ## $over_write_file_with ## [1] \"User-agent: *\\nAllow: /\" ##  ## $signal ## [1] \"warning\" ##  ## $cache ## [1] TRUE ##  ## $priority ## [1] 1 on_redirect_default ## $cache ## [1] TRUE ##  ## $priority ## [1] 3 on_domain_change_default ## $signal ## [1] \"warning\" ##  ## $cache ## [1] TRUE ##  ## $priority ## [1] 4 on_file_type_mismatch_default ## $over_write_file_with ## [1] \"User-agent: *\\nAllow: /\" ##  ## $signal ## [1] \"warning\" ##  ## $cache ## [1] TRUE ##  ## $priority ## [1] 6 on_suspect_content_default ## $over_write_file_with ## [1] \"User-agent: *\\nAllow: /\" ##  ## $signal ## [1] \"warning\" ##  ## $cache ## [1] TRUE ##  ## $priority ## [1] 7"},{"path":"https://docs.ropensci.org/robotstxt/index.html","id":"design-map-for-eventstate-handling","dir":"","previous_headings":"Usage","what":"Design Map for Event/State Handling","title":"A robots.txt Parser and Webbot/Spider/Crawler Permissions Checker","text":"version 0.7.x onwards previous releases concerned implementing parsing permission checking improving performance 0.7.x release robots.txt retrieval foremost. retrieval implemented corner cases retrieval stage well influence interpretation permissions granted. Features Problems handled: now handles corner cases retrieving robots.txt files e.g. robots.txt file available basically means “can scrape ” corner cases (server error, redirection takes place, redirection takes place different domains, file returned parsable, format HTML JSON, …) Design Decisions server error client error file found (404) redirection redirection another domain mime type / file type specification mismatch suspicious content (file content seem JSON, HTML, XML instead robots.txt) state/event handler define states events handled handler handler executes rules defined individual handlers handler can overwritten handler defaults defined always right thing overwrite content robots.txt file (e.g. allow/disallow ) modify problems signaled: error, warning, message, none robots.txt file retrieval cached transparency reacting post-mortem problems occured handler (even actual execution HTTP-request) can overwritten runtime inject user defined behaviour beforehand","code":""},{"path":"https://docs.ropensci.org/robotstxt/index.html","id":"warnings","dir":"","previous_headings":"Usage","what":"Warnings","title":"A robots.txt Parser and Webbot/Spider/Crawler Permissions Checker","text":"default functions retrieving robots.txt files warn HTTP events happening retrieving file (e.g. redirects) content file seem valid robots.txt file. warnings following example can turned three ways: (example) (solution 1) (solution 2) (solution 3)","code":"library(robotstxt)  paths_allowed(\"petermeissner.de\") ##  petermeissner.de ## [1] TRUE library(robotstxt)  suppressWarnings({   paths_allowed(\"petermeissner.de\") }) ##  petermeissner.de ## [1] TRUE library(robotstxt)  paths_allowed(\"petermeissner.de\", warn = FALSE) ##  petermeissner.de ## [1] TRUE library(robotstxt)  options(robotstxt_warn = FALSE)  paths_allowed(\"petermeissner.de\") ##  petermeissner.de ## [1] TRUE"},{"path":"https://docs.ropensci.org/robotstxt/index.html","id":"inspection-and-debugging","dir":"","previous_headings":"Usage","what":"Inspection and Debugging","title":"A robots.txt Parser and Webbot/Spider/Crawler Permissions Checker","text":"robots.txt files retrieved basically mere character vectors: last HTTP request stored object also additional information stored attributes. Events might change interpretation rules found robots.txt file: {httr} request-response object allwos dig exactly going client-server exchange. … lets us retrieve original content given back server: … look actual HTTP request issued response headers given back server:","code":"rt <-    get_robotstxt(\"petermeissner.de\")  as.character(rt) ## [1] \"# just do it - punk\\n\"  cat(rt) ## # just do it - punk rt_last_http$request ## Response [https://petermeissner.de/robots.txt] ##   Date: 2020-09-03 19:05 ##   Status: 200 ##   Content-Type: text/plain ##   Size: 20 B ## # just do it - punk names(attributes(rt)) ## [1] \"problems\" \"cached\"   \"request\"  \"class\" attr(rt, \"problems\") ## $on_redirect ## $on_redirect[[1]] ## $on_redirect[[1]]$status ## [1] 301 ##  ## $on_redirect[[1]]$location ## [1] \"https://petermeissner.de/robots.txt\" ##  ##  ## $on_redirect[[2]] ## $on_redirect[[2]]$status ## [1] 200 ##  ## $on_redirect[[2]]$location ## NULL attr(rt, \"request\") ## Response [https://petermeissner.de/robots.txt] ##   Date: 2020-09-03 19:05 ##   Status: 200 ##   Content-Type: text/plain ##   Size: 20 B ## # just do it - punk httr::content(   x        = attr(rt, \"request\"),    as       = \"text\",   encoding = \"UTF-8\" ) ## [1] \"# just do it - punk\\n\" # extract request-response object rt_req <-    attr(rt, \"request\")  # HTTP request rt_req$request ## <request> ## GET http://petermeissner.de/robots.txt ## Output: write_memory ## Options: ## * useragent: libcurl/7.64.1 r-curl/4.3 httr/1.4.1 ## * ssl_verifypeer: 1 ## * httpget: TRUE ## Headers: ## * Accept: application/json, text/xml, application/xml, */* ## * user-agent: R version 3.6.3 (2020-02-29)  # response headers rt_req$all_headers ## [[1]] ## [[1]]$status ## [1] 301 ##  ## [[1]]$version ## [1] \"HTTP/1.1\" ##  ## [[1]]$headers ## $server ## [1] \"nginx/1.10.3 (Ubuntu)\" ##  ## $date ## [1] \"Thu, 03 Sep 2020 19:05:45 GMT\" ##  ## $`content-type` ## [1] \"text/html\" ##  ## $`content-length` ## [1] \"194\" ##  ## $connection ## [1] \"keep-alive\" ##  ## $location ## [1] \"https://petermeissner.de/robots.txt\" ##  ## attr(,\"class\") ## [1] \"insensitive\" \"list\"        ##  ##  ## [[2]] ## [[2]]$status ## [1] 200 ##  ## [[2]]$version ## [1] \"HTTP/1.1\" ##  ## [[2]]$headers ## $server ## [1] \"nginx/1.10.3 (Ubuntu)\" ##  ## $date ## [1] \"Thu, 03 Sep 2020 19:05:45 GMT\" ##  ## $`content-type` ## [1] \"text/plain\" ##  ## $`content-length` ## [1] \"20\" ##  ## $`last-modified` ## [1] \"Thu, 03 Sep 2020 15:33:01 GMT\" ##  ## $connection ## [1] \"keep-alive\" ##  ## $etag ## [1] \"\\\"5f510cad-14\\\"\" ##  ## $`accept-ranges` ## [1] \"bytes\" ##  ## attr(,\"class\") ## [1] \"insensitive\" \"list\""},{"path":"https://docs.ropensci.org/robotstxt/index.html","id":"transformation","dir":"","previous_headings":"Usage","what":"Transformation","title":"A robots.txt Parser and Webbot/Spider/Crawler Permissions Checker","text":"convenience package also includes .list() method robots.txt files.","code":"as.list(rt) ## $content ## [1] \"# just do it - punk\\n\" ##  ## $robotstxt ## [1] \"# just do it - punk\\n\" ##  ## $problems ## $problems$on_redirect ## $problems$on_redirect[[1]] ## $problems$on_redirect[[1]]$status ## [1] 301 ##  ## $problems$on_redirect[[1]]$location ## [1] \"https://petermeissner.de/robots.txt\" ##  ##  ## $problems$on_redirect[[2]] ## $problems$on_redirect[[2]]$status ## [1] 200 ##  ## $problems$on_redirect[[2]]$location ## NULL ##  ##  ##  ##  ## $request ## Response [https://petermeissner.de/robots.txt] ##   Date: 2020-09-03 19:05 ##   Status: 200 ##   Content-Type: text/plain ##   Size: 20 B ## # just do it - punk"},{"path":"https://docs.ropensci.org/robotstxt/index.html","id":"caching","dir":"","previous_headings":"Usage","what":"Caching","title":"A robots.txt Parser and Webbot/Spider/Crawler Permissions Checker","text":"retrieval robots.txt files cached per R-session basis. Restarting R-session invalidate cache. Also using function parameter froce = TRUE force package re-retrieve robots.txt file.","code":"paths_allowed(\"petermeissner.de/I_want_to_scrape_this_now\", force = TRUE, verbose = TRUE) ##  petermeissner.de                      rt_robotstxt_http_getter: force http get ## [1] TRUE paths_allowed(\"petermeissner.de/I_want_to_scrape_this_now\",verbose = TRUE) ##  petermeissner.de                      rt_robotstxt_http_getter: cached http get ## [1] TRUE"},{"path":"https://docs.ropensci.org/robotstxt/index.html","id":"more-information","dir":"","previous_headings":"","what":"More information","title":"A robots.txt Parser and Webbot/Spider/Crawler Permissions Checker","text":"https://www.robotstxt.org/norobots-rfc.txt look vignette https://cran.r-project.org/package=robotstxt/vignettes/using_robotstxt.html Google robots.txt https://wiki.selfhtml.org/wiki/Grundlagen/Robots.txt https://support.google.com/webmasters/answer/6062608?hl=en https://www.robotstxt.org/robotstxt.html","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/as.list.robotstxt_text.html","id":null,"dir":"Reference","previous_headings":"","what":"Method as.list() for class robotstxt_text — as.list.robotstxt_text","title":"Method as.list() for class robotstxt_text — as.list.robotstxt_text","text":"Method .list() class robotstxt_text","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/as.list.robotstxt_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Method as.list() for class robotstxt_text — as.list.robotstxt_text","text":"","code":"# S3 method for robotstxt_text as.list(x, ...)"},{"path":"https://docs.ropensci.org/robotstxt/reference/as.list.robotstxt_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Method as.list() for class robotstxt_text — as.list.robotstxt_text","text":"x class robotstxt_text object transformed list ... arguments (inherited base::.list())","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/fix_url.html","id":null,"dir":"Reference","previous_headings":"","what":"fix_url — fix_url","title":"fix_url — fix_url","text":"fix_url","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/fix_url.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"fix_url — fix_url","text":"","code":"fix_url(url)"},{"path":"https://docs.ropensci.org/robotstxt/reference/fix_url.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"fix_url — fix_url","text":"url character string containing single URL","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/get_robotstxt.html","id":null,"dir":"Reference","previous_headings":"","what":"downloading robots.txt file — get_robotstxt","title":"downloading robots.txt file — get_robotstxt","text":"downloading robots.txt file","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/get_robotstxt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"downloading robots.txt file — get_robotstxt","text":"","code":"get_robotstxt(   domain,   warn = getOption(\"robotstxt_warn\", TRUE),   force = FALSE,   user_agent = utils::sessionInfo()$R.version$version.string,   ssl_verifypeer = c(1, 0),   encoding = \"UTF-8\",   verbose = FALSE,   rt_request_handler = robotstxt::rt_request_handler,   rt_robotstxt_http_getter = robotstxt::get_robotstxt_http_get,   on_server_error = on_server_error_default,   on_client_error = on_client_error_default,   on_not_found = on_not_found_default,   on_redirect = on_redirect_default,   on_domain_change = on_domain_change_default,   on_file_type_mismatch = on_file_type_mismatch_default,   on_suspect_content = on_suspect_content_default )"},{"path":"https://docs.ropensci.org/robotstxt/reference/get_robotstxt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"downloading robots.txt file — get_robotstxt","text":"domain domain download robots.txt file warn warn unable download domain/robots.txt force TRUE instead using possible cached results function re-download robotstxt file HTTP response status 404. happens, user_agent HTTP user-agent string used retrieve robots.txt file domain ssl_verifypeer analog CURL option https://curl.haxx.se/libcurl/c/CURLOPT_SSL_VERIFYPEER.html -- might help robots.txt file retrieval cases encoding Encoding robots.txt file. verbose make function print information rt_request_handler handler function handles request according event handlers specified rt_robotstxt_http_getter function executes HTTP request on_server_error request state handler 5xx status on_client_error request state handler 4xx HTTP status 404 on_not_found request state handler HTTP status 404 on_redirect request state handler 3xx HTTP status on_domain_change request state handler 3xx HTTP status domain change well on_file_type_mismatch request state handler content type 'text/plain' on_suspect_content request state handler content seems something else robots.txt file (usually JSON, XML HTML)","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/get_robotstxt_http_get.html","id":null,"dir":"Reference","previous_headings":"","what":"storage for http request response objects — rt_last_http","title":"storage for http request response objects — rt_last_http","text":"storage http request response objects get_robotstxt() worker function execute HTTP request","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/get_robotstxt_http_get.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"storage for http request response objects — rt_last_http","text":"","code":"rt_last_http  get_robotstxt_http_get(   domain,   user_agent = utils::sessionInfo()$R.version$version.string,   ssl_verifypeer = 1 )"},{"path":"https://docs.ropensci.org/robotstxt/reference/get_robotstxt_http_get.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"storage for http request response objects — rt_last_http","text":"object class environment length 1.","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/get_robotstxt_http_get.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"storage for http request response objects — rt_last_http","text":"domain domain get tobots.txt. file user_agent user agent use HTTP request header ssl_verifypeer analog CURL option https://curl.haxx.se/libcurl/c/CURLOPT_SSL_VERIFYPEER.html -- might help robots.txt file retrieval cases","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/get_robotstxts.html","id":null,"dir":"Reference","previous_headings":"","what":"function to get multiple robotstxt files — get_robotstxts","title":"function to get multiple robotstxt files — get_robotstxts","text":"function get multiple robotstxt files","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/get_robotstxts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"function to get multiple robotstxt files — get_robotstxts","text":"","code":"get_robotstxts(   domain,   warn = TRUE,   force = FALSE,   user_agent = utils::sessionInfo()$R.version$version.string,   ssl_verifypeer = c(1, 0),   use_futures = FALSE,   verbose = FALSE,   rt_request_handler = robotstxt::rt_request_handler,   rt_robotstxt_http_getter = robotstxt::get_robotstxt_http_get,   on_server_error = on_server_error_default,   on_client_error = on_client_error_default,   on_not_found = on_not_found_default,   on_redirect = on_redirect_default,   on_domain_change = on_domain_change_default,   on_file_type_mismatch = on_file_type_mismatch_default,   on_suspect_content = on_suspect_content_default )"},{"path":"https://docs.ropensci.org/robotstxt/reference/get_robotstxts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"function to get multiple robotstxt files — get_robotstxts","text":"domain domain download robots.txt file warn warn unable download domain/robots.txt force TRUE instead using possible cached results function re-download robotstxt file HTTP response status 404. happens, user_agent HTTP user-agent string used retrieve robots.txt file domain ssl_verifypeer analog CURL option https://curl.haxx.se/libcurl/c/CURLOPT_SSL_VERIFYPEER.html -- might help robots.txt file retrieval cases use_futures future::future_lapply used possible parallel/async retrieval . Note: check help pages vignettes package future set plans future execution robotstxt package . verbose make function print information rt_request_handler handler function handles request according event handlers specified rt_robotstxt_http_getter function executes HTTP request on_server_error request state handler 5xx status on_client_error request state handler 4xx HTTP status 404 on_not_found request state handler HTTP status 404 on_redirect request state handler 3xx HTTP status on_domain_change request state handler 3xx HTTP status domain change well on_file_type_mismatch request state handler content type 'text/plain' on_suspect_content request state handler content seems something else robots.txt file (usually JSON, XML HTML)","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/guess_domain.html","id":null,"dir":"Reference","previous_headings":"","what":"function guessing domain from path — guess_domain","title":"function guessing domain from path — guess_domain","text":"function guessing domain path","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/guess_domain.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"function guessing domain from path — guess_domain","text":"","code":"guess_domain(x)"},{"path":"https://docs.ropensci.org/robotstxt/reference/guess_domain.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"function guessing domain from path — guess_domain","text":"x path aka URL infer domain","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/http_domain_changed.html","id":null,"dir":"Reference","previous_headings":"","what":"http_domain_changed — http_domain_changed","title":"http_domain_changed — http_domain_changed","text":"http_domain_changed","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/http_domain_changed.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"http_domain_changed — http_domain_changed","text":"","code":"http_domain_changed(response)"},{"path":"https://docs.ropensci.org/robotstxt/reference/http_domain_changed.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"http_domain_changed — http_domain_changed","text":"response httr response object, e.g. call httr::GET()","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/http_domain_changed.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"http_domain_changed — http_domain_changed","text":"logical length 1 indicating whether domain change     happened HTTP request","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/http_subdomain_changed.html","id":null,"dir":"Reference","previous_headings":"","what":"http_subdomain_changed — http_subdomain_changed","title":"http_subdomain_changed — http_subdomain_changed","text":"http_subdomain_changed","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/http_subdomain_changed.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"http_subdomain_changed — http_subdomain_changed","text":"","code":"http_subdomain_changed(response)"},{"path":"https://docs.ropensci.org/robotstxt/reference/http_subdomain_changed.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"http_subdomain_changed — http_subdomain_changed","text":"response httr response object, e.g. call httr::GET()","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/http_subdomain_changed.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"http_subdomain_changed — http_subdomain_changed","text":"logical length 1 indicating whether domain change     happened HTTP request","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/http_was_redirected.html","id":null,"dir":"Reference","previous_headings":"","what":"http_was_redirected — http_was_redirected","title":"http_was_redirected — http_was_redirected","text":"http_was_redirected","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/http_was_redirected.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"http_was_redirected — http_was_redirected","text":"","code":"http_was_redirected(response)"},{"path":"https://docs.ropensci.org/robotstxt/reference/http_was_redirected.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"http_was_redirected — http_was_redirected","text":"response httr response object, e.g. call httr::GET()","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/http_was_redirected.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"http_was_redirected — http_was_redirected","text":"logical length 1 indicating whether redirect happened   HTTP request","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/is_suspect_robotstxt.html","id":null,"dir":"Reference","previous_headings":"","what":"is_suspect_robotstxt — is_suspect_robotstxt","title":"is_suspect_robotstxt — is_suspect_robotstxt","text":"function checks file valid / parsable robots.txt file","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/is_suspect_robotstxt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"is_suspect_robotstxt — is_suspect_robotstxt","text":"","code":"is_suspect_robotstxt(text)"},{"path":"https://docs.ropensci.org/robotstxt/reference/is_suspect_robotstxt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"is_suspect_robotstxt — is_suspect_robotstxt","text":"text content robots.txt file provides character vector","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/is_valid_robotstxt.html","id":null,"dir":"Reference","previous_headings":"","what":"function that checks if file is valid / parsable robots.txt file — is_valid_robotstxt","title":"function that checks if file is valid / parsable robots.txt file — is_valid_robotstxt","text":"function checks file valid / parsable robots.txt file","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/is_valid_robotstxt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"function that checks if file is valid / parsable robots.txt file — is_valid_robotstxt","text":"","code":"is_valid_robotstxt(text, check_strickt_ascii = FALSE)"},{"path":"https://docs.ropensci.org/robotstxt/reference/is_valid_robotstxt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"function that checks if file is valid / parsable robots.txt file — is_valid_robotstxt","text":"text content robots.txt file provides character vector check_strickt_ascii whether check content adhere specification RFC use plain text aka ASCII","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/list_merge.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge a number of named lists in sequential order — list_merge","title":"Merge a number of named lists in sequential order — list_merge","text":"Merge number named lists sequential order","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/list_merge.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge a number of named lists in sequential order — list_merge","text":"","code":"list_merge(...)"},{"path":"https://docs.ropensci.org/robotstxt/reference/list_merge.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge a number of named lists in sequential order — list_merge","text":"... named lists","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/list_merge.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Merge a number of named lists in sequential order — list_merge","text":"List merging usually useful merging program settings configuraion multiple versions across time, multiple administrative levels. example, program settings may initial version keys defined specified. later versions, partial modifications recorded. case, list merging can useful merge versions settings release order versions. result fully updated settings later modifications applied.","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/list_merge.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Merge a number of named lists in sequential order — list_merge","text":"Kun Ren <mail@renkun.> function merges number lists sequential order modifyList, , later list always modifies former list form merged list, resulted list merged next list. process repeated lists ... list exausted.","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/named_list.html","id":null,"dir":"Reference","previous_headings":"","what":"make automatically named list — named_list","title":"make automatically named list — named_list","text":"make automatically named list","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/named_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"make automatically named list — named_list","text":"","code":"named_list(...)"},{"path":"https://docs.ropensci.org/robotstxt/reference/named_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"make automatically named list — named_list","text":"... things put list","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/null_to_defeault.html","id":null,"dir":"Reference","previous_headings":"","what":"null_to_defeault — null_to_defeault","title":"null_to_defeault — null_to_defeault","text":"null_to_defeault","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/null_to_defeault.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"null_to_defeault — null_to_defeault","text":"","code":"null_to_defeault(x, d)"},{"path":"https://docs.ropensci.org/robotstxt/reference/null_to_defeault.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"null_to_defeault — null_to_defeault","text":"x value check return d value return case x NULL","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/parse_robotstxt.html","id":null,"dir":"Reference","previous_headings":"","what":"function parsing robots.txt — parse_robotstxt","title":"function parsing robots.txt — parse_robotstxt","text":"function parsing robots.txt","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/parse_robotstxt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"function parsing robots.txt — parse_robotstxt","text":"","code":"parse_robotstxt(txt)"},{"path":"https://docs.ropensci.org/robotstxt/reference/parse_robotstxt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"function parsing robots.txt — parse_robotstxt","text":"txt content robots.txt file","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/parse_robotstxt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"function parsing robots.txt — parse_robotstxt","text":"named list useragents, comments, permissions, sitemap","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/parse_url.html","id":null,"dir":"Reference","previous_headings":"","what":"parse_url — parse_url","title":"parse_url — parse_url","text":"parse_url","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/parse_url.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"parse_url — parse_url","text":"","code":"parse_url(url)"},{"path":"https://docs.ropensci.org/robotstxt/reference/parse_url.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"parse_url — parse_url","text":"url url parse components","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/parse_url.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"parse_url — parse_url","text":"data.frame columns protocol, domain, path","code":""},{"path":[]},{"path":"https://docs.ropensci.org/robotstxt/reference/paths_allowed.html","id":null,"dir":"Reference","previous_headings":"","what":"check if a bot has permissions to access page(s) — paths_allowed","title":"check if a bot has permissions to access page(s) — paths_allowed","text":"check bot permissions access page(s)","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/paths_allowed.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"check if a bot has permissions to access page(s) — paths_allowed","text":"","code":"paths_allowed(   paths = \"/\",   domain = \"auto\",   bot = \"*\",   user_agent = utils::sessionInfo()$R.version$version.string,   check_method = c(\"spiderbar\"),   warn = getOption(\"robotstxt_warn\", TRUE),   force = FALSE,   ssl_verifypeer = c(1, 0),   use_futures = TRUE,   robotstxt_list = NULL,   verbose = FALSE,   rt_request_handler = robotstxt::rt_request_handler,   rt_robotstxt_http_getter = robotstxt::get_robotstxt_http_get,   on_server_error = on_server_error_default,   on_client_error = on_client_error_default,   on_not_found = on_not_found_default,   on_redirect = on_redirect_default,   on_domain_change = on_domain_change_default,   on_file_type_mismatch = on_file_type_mismatch_default,   on_suspect_content = on_suspect_content_default )"},{"path":"https://docs.ropensci.org/robotstxt/reference/paths_allowed.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"check if a bot has permissions to access page(s) — paths_allowed","text":"paths paths check bot's permission, defaults \"/\". Please, note path folder end trailing slash (\"/\"). domain Domain paths checked. Defaults \"auto\". set \"auto\" function try guess domain parsing paths argument. Note however, educated guesses might utterly fail. safe side, provide appropriate domains manually. bot name bot, defaults \"*\" user_agent HTTP user-agent string used retrieve robots.txt file domain check_method moment kept backward compatibility reasons - use parameter anymore --> let function simply use default warn suppress warnings force TRUE instead using possible cached results function re-download robotstxt file HTTP response status 404. happens, ssl_verifypeer analog CURL option https://curl.haxx.se/libcurl/c/CURLOPT_SSL_VERIFYPEER.html -- might help robots.txt file retrieval cases use_futures future::future_lapply used possible parallel/async retrieval . Note: check help pages vignettes package future set plans future execution robotstxt package . robotstxt_list either NULL -- default -- list character vectors one vector per path check verbose make function print information rt_request_handler handler function handles request according event handlers specified rt_robotstxt_http_getter function executes HTTP request on_server_error request state handler 5xx status on_client_error request state handler 4xx HTTP status 404 on_not_found request state handler HTTP status 404 on_redirect request state handler 3xx HTTP status on_domain_change request state handler 3xx HTTP status domain change well on_file_type_mismatch request state handler content type 'text/plain' on_suspect_content request state handler content seems something else robots.txt file (usually JSON, XML HTML)","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/paths_allowed_worker_spiderbar.html","id":null,"dir":"Reference","previous_headings":"","what":"paths_allowed_worker spiderbar flavor — paths_allowed_worker_spiderbar","title":"paths_allowed_worker spiderbar flavor — paths_allowed_worker_spiderbar","text":"paths_allowed_worker spiderbar flavor","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/paths_allowed_worker_spiderbar.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"paths_allowed_worker spiderbar flavor — paths_allowed_worker_spiderbar","text":"","code":"paths_allowed_worker_spiderbar(domain, bot, paths, robotstxt_list)"},{"path":"https://docs.ropensci.org/robotstxt/reference/paths_allowed_worker_spiderbar.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"paths_allowed_worker spiderbar flavor — paths_allowed_worker_spiderbar","text":"domain Domain paths checked. Defaults \"auto\". set \"auto\" function try guess domain parsing paths argument. Note however, educated guesses might utterly fail. safe side, provide appropriate domains manually. bot name bot, defaults \"*\" paths paths check bot's permission, defaults \"/\". Please, note path folder end trailing slash (\"/\"). robotstxt_list either NULL -- default -- list character vectors one vector per path check","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"re-export magrittr pipe operator — %>%","title":"re-export magrittr pipe operator — %>%","text":"re-export magrittr pipe operator","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/print.robotstxt.html","id":null,"dir":"Reference","previous_headings":"","what":"printing robotstxt — print.robotstxt","title":"printing robotstxt — print.robotstxt","text":"printing robotstxt","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/print.robotstxt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"printing robotstxt — print.robotstxt","text":"","code":"# S3 method for robotstxt print(x, ...)"},{"path":"https://docs.ropensci.org/robotstxt/reference/print.robotstxt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"printing robotstxt — print.robotstxt","text":"x robotstxt instance printed ... goes sink","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/print.robotstxt_text.html","id":null,"dir":"Reference","previous_headings":"","what":"printing robotstxt_text — print.robotstxt_text","title":"printing robotstxt_text — print.robotstxt_text","text":"printing robotstxt_text","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/print.robotstxt_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"printing robotstxt_text — print.robotstxt_text","text":"","code":"# S3 method for robotstxt_text print(x, ...)"},{"path":"https://docs.ropensci.org/robotstxt/reference/print.robotstxt_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"printing robotstxt_text — print.robotstxt_text","text":"x character vector aka robotstxt$text printed ... goes sink","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/remove_domain.html","id":null,"dir":"Reference","previous_headings":"","what":"function to remove domain from path — remove_domain","title":"function to remove domain from path — remove_domain","text":"function remove domain path","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/remove_domain.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"function to remove domain from path — remove_domain","text":"","code":"remove_domain(x)"},{"path":"https://docs.ropensci.org/robotstxt/reference/remove_domain.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"function to remove domain from path — remove_domain","text":"x path aka URL first infer domain remove ","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/request_handler_handler.html","id":null,"dir":"Reference","previous_headings":"","what":"request_handler_handler — request_handler_handler","title":"request_handler_handler — request_handler_handler","text":"Helper function handle robotstxt handlers.","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/request_handler_handler.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"request_handler_handler — request_handler_handler","text":"","code":"request_handler_handler(request, handler, res, info = TRUE, warn = TRUE)"},{"path":"https://docs.ropensci.org/robotstxt/reference/request_handler_handler.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"request_handler_handler — request_handler_handler","text":"request request object returned call httr::GET() handler handler either character string entailing various options function producing specific list, see return. res list list elements '[handler names], ...', 'rtxt', 'cache' info info add problems list warn FALSE warnings messages suppressed","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/request_handler_handler.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"request_handler_handler — request_handler_handler","text":"list elements '[handler name]', 'rtxt', 'cache'","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/robotstxt.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a representations of a robots.txt file — robotstxt","title":"Generate a representations of a robots.txt file — robotstxt","text":"function generates list entails data resulting parsing robots.txt file well function called check enables ask representation bot (particular bots) allowed access resource domain.","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/robotstxt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a representations of a robots.txt file — robotstxt","text":"","code":"robotstxt(   domain = NULL,   text = NULL,   user_agent = NULL,   warn = getOption(\"robotstxt_warn\", TRUE),   force = FALSE,   ssl_verifypeer = c(1, 0),   encoding = \"UTF-8\",   verbose = FALSE,   on_server_error = on_server_error_default,   on_client_error = on_client_error_default,   on_not_found = on_not_found_default,   on_redirect = on_redirect_default,   on_domain_change = on_domain_change_default,   on_file_type_mismatch = on_file_type_mismatch_default,   on_suspect_content = on_suspect_content_default )"},{"path":"https://docs.ropensci.org/robotstxt/reference/robotstxt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a representations of a robots.txt file — robotstxt","text":"domain Domain generate representation. text equals NULL, function download file server - default. text automatic download robots.txt preferred, text can supplied directly. user_agent HTTP user-agent string used retrieve robots.txt file domain warn warn unable download domain/robots.txt force TRUE instead using possible cached results function re-download robotstxt file HTTP response status 404. happens, ssl_verifypeer analog CURL option https://curl.haxx.se/libcurl/c/CURLOPT_SSL_VERIFYPEER.html -- might help robots.txt file retrieval cases encoding Encoding robots.txt file. verbose make function print information on_server_error request state handler 5xx status on_client_error request state handler 4xx HTTP status 404 on_not_found request state handler HTTP status 404 on_redirect request state handler 3xx HTTP status on_domain_change request state handler 3xx HTTP status domain change well on_file_type_mismatch request state handler content type 'text/plain' on_suspect_content request state handler content seems something else robots.txt file (usually JSON, XML HTML)","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/robotstxt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a representations of a robots.txt file — robotstxt","text":"Object (list) class robotstxt parsed data   robots.txt (domain, text, bots, permissions, host, sitemap, ) one   function (check()) check resource permissions.","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/robotstxt.html","id":"fields","dir":"Reference","previous_headings":"","what":"Fields","title":"Generate a representations of a robots.txt file — robotstxt","text":"domain character vector holding domain name robots.txt file valid; set NA supplied initialization text character vector text robots.txt file; either supplied initialization automatically downloaded domain supplied initialization bots character vector bot names mentioned robots.txt permissions data.frame bot permissions found robots.txt file host data.frame host fields found robots.txt file sitemap data.frame sitemap fields found robots.txt file data.frame - none - fields found robots.txt file check() Method check bot permissions. Defaults domains root bot particular. check() two arguments: paths bot. first supplying paths check permissions latter put name bot. Please, note path folder end trailing slash (\"/\").","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/robotstxt.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate a representations of a robots.txt file — robotstxt","text":"","code":"if (FALSE) { rt <- robotstxt(domain=\"google.com\") rt$bots rt$permissions rt$check( paths = c(\"/\", \"forbidden\"), bot=\"*\") }"},{"path":"https://docs.ropensci.org/robotstxt/reference/rt_cache.html","id":null,"dir":"Reference","previous_headings":"","what":"get_robotstxt() cache — rt_cache","title":"get_robotstxt() cache — rt_cache","text":"get_robotstxt() cache","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/rt_cache.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"get_robotstxt() cache — rt_cache","text":"","code":"rt_cache"},{"path":"https://docs.ropensci.org/robotstxt/reference/rt_cache.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"get_robotstxt() cache — rt_cache","text":"object class environment length 0.","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/rt_get_comments.html","id":null,"dir":"Reference","previous_headings":"","what":"extracting comments from robots.txt — rt_get_comments","title":"extracting comments from robots.txt — rt_get_comments","text":"extracting comments robots.txt","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/rt_get_comments.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"extracting comments from robots.txt — rt_get_comments","text":"","code":"rt_get_comments(txt)"},{"path":"https://docs.ropensci.org/robotstxt/reference/rt_get_comments.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"extracting comments from robots.txt — rt_get_comments","text":"txt content robots.txt file","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/rt_get_fields.html","id":null,"dir":"Reference","previous_headings":"","what":"extracting permissions from robots.txt — rt_get_fields","title":"extracting permissions from robots.txt — rt_get_fields","text":"extracting permissions robots.txt","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/rt_get_fields.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"extracting permissions from robots.txt — rt_get_fields","text":"","code":"rt_get_fields(txt, regex = \"\", invert = FALSE)"},{"path":"https://docs.ropensci.org/robotstxt/reference/rt_get_fields.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"extracting permissions from robots.txt — rt_get_fields","text":"txt content robots.txt file regex regular expression specify field invert invert selection made via regex?","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/rt_get_fields_worker.html","id":null,"dir":"Reference","previous_headings":"","what":"extracting robotstxt fields — rt_get_fields_worker","title":"extracting robotstxt fields — rt_get_fields_worker","text":"extracting robotstxt fields","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/rt_get_fields_worker.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"extracting robotstxt fields — rt_get_fields_worker","text":"","code":"rt_get_fields_worker(txt, type = \"all\", regex = NULL, invert = FALSE)"},{"path":"https://docs.ropensci.org/robotstxt/reference/rt_get_fields_worker.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"extracting robotstxt fields — rt_get_fields_worker","text":"txt content robots.txt file type name names fields returned, defaults fields regex subsetting field names via regular expressions invert field selection","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/rt_get_rtxt.html","id":null,"dir":"Reference","previous_headings":"","what":"load robots.txt files saved along with the package — rt_get_rtxt","title":"load robots.txt files saved along with the package — rt_get_rtxt","text":"load robots.txt files saved along package: functions handy testing (used otherwise)","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/rt_get_rtxt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"load robots.txt files saved along with the package — rt_get_rtxt","text":"","code":"rt_get_rtxt(name = sample(rt_list_rtxt(), 1))"},{"path":"https://docs.ropensci.org/robotstxt/reference/rt_get_rtxt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"load robots.txt files saved along with the package — rt_get_rtxt","text":"name name robots.txt files, defaults random drawn file ;-)","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/rt_get_useragent.html","id":null,"dir":"Reference","previous_headings":"","what":"extracting HTTP useragents from robots.txt — rt_get_useragent","title":"extracting HTTP useragents from robots.txt — rt_get_useragent","text":"extracting HTTP useragents robots.txt","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/rt_get_useragent.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"extracting HTTP useragents from robots.txt — rt_get_useragent","text":"","code":"rt_get_useragent(txt)"},{"path":"https://docs.ropensci.org/robotstxt/reference/rt_get_useragent.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"extracting HTTP useragents from robots.txt — rt_get_useragent","text":"txt content robots.txt file","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/rt_list_rtxt.html","id":null,"dir":"Reference","previous_headings":"","what":"list robots.txt files saved along with the package — rt_list_rtxt","title":"list robots.txt files saved along with the package — rt_list_rtxt","text":"list robots.txt files saved along package: functions ar handy testing (used otherwise)","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/rt_list_rtxt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"list robots.txt files saved along with the package — rt_list_rtxt","text":"","code":"rt_list_rtxt()"},{"path":"https://docs.ropensci.org/robotstxt/reference/rt_request_handler.html","id":null,"dir":"Reference","previous_headings":"","what":"rt_request_handler — rt_request_handler","title":"rt_request_handler — rt_request_handler","text":"helper function get_robotstxt() extract robots.txt file HTTP request result object. furthermore inform get_robotstxt() request cached problems occured.","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/rt_request_handler.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"rt_request_handler — rt_request_handler","text":"","code":"rt_request_handler(   request,   on_server_error = on_server_error_default,   on_client_error = on_client_error_default,   on_not_found = on_not_found_default,   on_redirect = on_redirect_default,   on_domain_change = on_domain_change_default,   on_sub_domain_change = on_sub_domain_change_default,   on_file_type_mismatch = on_file_type_mismatch_default,   on_suspect_content = on_suspect_content_default,   warn = TRUE,   encoding = \"UTF-8\" )  on_server_error_default  on_client_error_default  on_not_found_default  on_redirect_default  on_domain_change_default  on_sub_domain_change_default  on_file_type_mismatch_default  on_suspect_content_default"},{"path":"https://docs.ropensci.org/robotstxt/reference/rt_request_handler.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"rt_request_handler — rt_request_handler","text":"object class list length 4. object class list length 4. object class list length 4. object class list length 2. object class list length 3. object class list length 2. object class list length 4. object class list length 4.","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/rt_request_handler.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"rt_request_handler — rt_request_handler","text":"request result HTTP request (e.g. httr::GET()) on_server_error request state handler 5xx status on_client_error request state handler 4xx HTTP status 404 on_not_found request state handler HTTP status 404 on_redirect request state handler 3xx HTTP status on_domain_change request state handler 3xx HTTP status domain change well on_sub_domain_change request state handler 3xx HTTP status domain change www-sub_domain on_file_type_mismatch request state handler content type 'text/plain' on_suspect_content request state handler content seems something else robots.txt file (usually JSON, XML HTML) warn suppress warnings encoding text encoding assume encoding provided headers response","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/rt_request_handler.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"rt_request_handler — rt_request_handler","text":"list three items following following schema:  list( rtxt = \"\", problems = list( \"redirect\" = list( status_code = 301 ),   \"domain\" = list(from_url = \"...\", to_url = \"...\") ) )","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/sanitize_path.html","id":null,"dir":"Reference","previous_headings":"","what":"making paths uniform — sanitize_path","title":"making paths uniform — sanitize_path","text":"making paths uniform","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/sanitize_path.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"making paths uniform — sanitize_path","text":"","code":"sanitize_path(path)"},{"path":"https://docs.ropensci.org/robotstxt/reference/sanitize_path.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"making paths uniform — sanitize_path","text":"path path sanitized","code":""},{"path":"https://docs.ropensci.org/robotstxt/reference/sanitize_path.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"making paths uniform — sanitize_path","text":"sanitized path","code":""}]
