<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Provides functions to download and parse robots.txt files.
        Ultimately the package makes it easy to check if bots
        (spiders, crawler, scrapers, ...) are allowed to access specific
        resources on a domain.">
<title>A robots.txt Parser and Webbot/Spider/Crawler Permissions Checker • robotstxt</title>
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="/apple-touch-icon.png">
<script src="deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://docs.ropensci.org/bootstrap-toc.min.js"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="pkgdown.js"></script><meta property="og:title" content="A robots.txt Parser and Webbot/Spider/Crawler Permissions Checker">
<meta property="og:description" content="Provides functions to download and parse robots.txt files.
        Ultimately the package makes it easy to check if bots
        (spiders, crawler, scrapers, ...) are allowed to access specific
        resources on a domain.">
<meta property="og:image" content="https://docs.ropensci.org/robotstxt/logo.png">
<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--><!-- Matomo --><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/cookieconsent@3/build/cookieconsent.min.css">
<script src="https://cdn.jsdelivr.net/npm/cookieconsent@3/build/cookieconsent.min.js" data-cfasync="false"></script><script src="https://ropensci.org/scripts/matomo.js"></script><noscript><p><img src="https://ropensci.matomo.cloud/matomo.php?idsite=1&amp;rec=1" style="border:0;" alt=""></p></noscript>
<!-- End Matomo Code -->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    <a href="https://ropensci.org" class="external-link"><img src="https://ropensci.org/img/icon_short_white.svg" id="hexlogo" alt="rOpenSci"></a>
    <a class="navbar-brand me-2" href="index.html">robotstxt</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Released version">0.7.13</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"></ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="nav-link" href="reference/index.html">Reference</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="articles/using_robotstxt.html">Using Robotstxt</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="news/index.html">Changelog</a>
</li>
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/ropensci/robotstxt/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-home">
<div class="row">
  <main id="main" class="col-md-9"><div class="section level2">
<h2 id="a-robotstxt-parser-and-webbotspidercrawler-permissions-checker">A ‘robots.txt’ Parser and ‘Webbot’/‘Spider’/‘Crawler’ Permissions Checker<a class="anchor" aria-label="anchor" href="#a-robotstxt-parser-and-webbotspidercrawler-permissions-checker"></a>
</h2>
<p><strong>Status</strong></p>
<p><em>lines of R code:</em> 1007, <em>lines of test code:</em> 1758</p>
<p><a href="https://www.repostatus.org/" class="external-link"><img src="https://www.repostatus.org/badges/latest/active.svg" alt="Project Status: Active – The project has reached a stable, usable state and is being actively developed."></a> <a href="https://github.com/ropensci/software-review/issues/25" class="external-link"><img src="https://badges.ropensci.org/25_status.svg"></a> <a href="https://travis-ci.org/ropensci/robotstxt" class="external-link"><img src="https://api.travis-ci.org/ropensci/robotstxt.svg?branch=master"></a><a></a> <a href="https://cran.r-project.org/package=robotstxt" class="external-link"><img src="http://www.r-pkg.org/badges/version/robotstxt"></a> <a href="https://cran.r-project.org/web/checks/check_results_reshape.html" class="external-link"><img src="https://cranchecks.info/badges/summary/reshape" alt="cran checks"></a> <a href="https://codecov.io/gh/ropensci/robotstxt" class="external-link"><img src="https://codecov.io/gh/ropensci/robotstxt/branch/master/graph/badge.svg" alt="Codecov"></a> <img src="http://cranlogs.r-pkg.org/badges/grand-total/robotstxt"><img src="http://cranlogs.r-pkg.org/badges/robotstxt"></p>
<p><strong>Development version</strong></p>
<p>0.7.13 - 2020-08-19 / 20:39:24</p>
<p><strong>Description</strong></p>
<p>Provides functions to download and parse ‘robots.txt’ files. Ultimately the package makes it easy to check if bots (spiders, crawler, scrapers, …) are allowed to access specific resources on a domain.</p>
<p><strong>License</strong></p>
<p>MIT + file LICENSE <br>Peter Meissner [aut, cre], Kun Ren [aut, cph] (Author and copyright holder of list_merge.R.), Oliver Keys [ctb] (original release code review), Rich Fitz John [ctb] (original release code review)</p>
<p><strong>Citation</strong></p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/citation.html" class="external-link">citation</a></span><span class="op">(</span><span class="st">"robotstxt"</span><span class="op">)</span></span></code></pre></div>
<p><strong>BibTex for citing</strong></p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/toLatex.html" class="external-link">toBibtex</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/utils/citation.html" class="external-link">citation</a></span><span class="op">(</span><span class="st">"robotstxt"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><strong>Contribution - AKA The-Think-Twice-Be-Nice-Rule</strong></p>
<p>Please note that this project is released with a Contributor Code of Conduct. By participating in this project you agree to abide by its terms:</p>
<blockquote>
<p>As contributors and maintainers of this project, we pledge to respect all people who contribute through reporting issues, posting feature requests, updating documentation, submitting pull requests or patches, and other activities.</p>
<p>We are committed to making participation in this project a harassment-free experience for everyone, regardless of level of experience, gender, gender identity and expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, or religion.</p>
<p>Examples of unacceptable behavior by participants include the use of sexual language or imagery, derogatory comments or personal attacks, trolling, public or private harassment, insults, or other unprofessional conduct.</p>
<p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct. Project maintainers who do not follow the Code of Conduct may be removed from the project team.</p>
<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by opening an issue or contacting one or more of the project maintainers.</p>
<p>This Code of Conduct is adapted from the Contributor Covenant (<a href="https://www.contributor-covenant.org/" class="external-link uri">https://www.contributor-covenant.org/</a>), version 1.0.0, available at <a href="https://www.contributor-covenant.org/version/1/0/0/code-of-conduct/" class="external-link uri">https://www.contributor-covenant.org/version/1/0/0/code-of-conduct/</a></p>
</blockquote>
</div>
<div class="section level2">
<h2 id="installation">Installation<a class="anchor" aria-label="anchor" href="#installation"></a>
</h2>
<p><strong>Installation and start - stable version</strong></p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html" class="external-link">install.packages</a></span><span class="op">(</span><span class="st">"robotstxt"</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://docs.ropensci.org/robotstxt/">robotstxt</a></span><span class="op">)</span></span></code></pre></div>
<p><strong>Installation and start - development version</strong></p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">devtools</span><span class="fu">::</span><span class="fu">install_github</span><span class="op">(</span><span class="st">"ropensci/robotstxt"</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://docs.ropensci.org/robotstxt/">robotstxt</a></span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="usage">Usage<a class="anchor" aria-label="anchor" href="#usage"></a>
</h2>
<p><strong>Robotstxt class documentation</strong></p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="op">?</span><span class="va">robotstxt</span></span></code></pre></div>
<p>Simple path access right checking (the functional way) …</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://docs.ropensci.org/robotstxt/">robotstxt</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/options.html" class="external-link">options</a></span><span class="op">(</span>robotstxt_warn <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="fu"><a href="reference/paths_allowed.html">paths_allowed</a></span><span class="op">(</span></span>
<span>  paths  <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"/api/rest_v1/?doc"</span>, <span class="st">"/w/"</span><span class="op">)</span>, </span>
<span>  domain <span class="op">=</span> <span class="st">"wikipedia.org"</span>, </span>
<span>  bot    <span class="op">=</span> <span class="st">"*"</span></span>
<span><span class="op">)</span></span>
<span><span class="co">##  wikipedia.org</span></span>
<span><span class="co">## [1]  TRUE FALSE</span></span>
<span></span>
<span><span class="fu"><a href="reference/paths_allowed.html">paths_allowed</a></span><span class="op">(</span></span>
<span>  paths <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span></span>
<span>    <span class="st">"https://wikipedia.org/api/rest_v1/?doc"</span>, </span>
<span>    <span class="st">"https://wikipedia.org/w/"</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="co">##  wikipedia.org                       wikipedia.org</span></span>
<span><span class="co">## [1]  TRUE FALSE</span></span></code></pre></div>
<p>… or (the object oriented way) …</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://docs.ropensci.org/robotstxt/">robotstxt</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/options.html" class="external-link">options</a></span><span class="op">(</span>robotstxt_warn <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span></span>
<span><span class="va">rtxt</span> <span class="op">&lt;-</span> </span>
<span>  <span class="fu"><a href="reference/robotstxt.html">robotstxt</a></span><span class="op">(</span>domain <span class="op">=</span> <span class="st">"wikipedia.org"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">rtxt</span><span class="op">$</span><span class="fu">check</span><span class="op">(</span></span>
<span>  paths <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"/api/rest_v1/?doc"</span>, <span class="st">"/w/"</span><span class="op">)</span>, </span>
<span>  bot   <span class="op">=</span> <span class="st">"*"</span></span>
<span><span class="op">)</span></span>
<span><span class="co">## [1]  TRUE FALSE</span></span></code></pre></div>
<div class="section level3">
<h3 id="retrieval">Retrieval<a class="anchor" aria-label="anchor" href="#retrieval"></a>
</h3>
<p>Retrieving the robots.txt file for a domain:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># retrieval</span></span>
<span><span class="va">rt</span> <span class="op">&lt;-</span> </span>
<span>  <span class="fu"><a href="reference/get_robotstxt.html">get_robotstxt</a></span><span class="op">(</span><span class="st">"https://petermeissner.de"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># printing</span></span>
<span><span class="va">rt</span></span>
<span><span class="co">## [robots.txt]</span></span>
<span><span class="co">## --------------------------------------</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## # just do it - punk</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="interpretation">Interpretation<a class="anchor" aria-label="anchor" href="#interpretation"></a>
</h3>
<p>Checking whether or not one is supposadly allowed to access some resource from a web server is - unfortunately - not just a matter of downloading and parsing a simple robots.txt file.</p>
<p>First there is no official specification for robots.txt files so every robots.txt file written and every robots.txt file read and used is an interpretation. Most of the time we all have a common understanding on how things are supposed to work but things get more complicated at the edges.</p>
<p>Some interpretation problems:</p>
<ul>
<li>finding no robots.txt file at the server (e.g. HTTP status code 404) implies that everything is allowed</li>
<li>subdomains should have there own robots.txt file if not it is assumed that everything is allowed</li>
<li>redirects involving protocol changes - e.g. upgrading from http to https - are followed and considered no domain or subdomain change - so whatever is found at the end of the redirect is considered to be the robots.txt file for the original domain</li>
<li>redirects from subdomain www to the doamin is considered no domain change - so whatever is found at the end of the redirect is considered to be the robots.txt file for the subdomain originally requested</li>
</ul>
</div>
<div class="section level3">
<h3 id="event-handling">Event Handling<a class="anchor" aria-label="anchor" href="#event-handling"></a>
</h3>
<p>Because the interpretation of robots.txt rules not just depends on the rules specified within the file, the package implements an event handler system that allows to interpret and re-interpret events into rules.</p>
<p>Under the hood the <code><a href="reference/rt_request_handler.html">rt_request_handler()</a></code> function is called within <code><a href="reference/get_robotstxt.html">get_robotstxt()</a></code>. This function takes an {httr} request-response object and a set of event handlers. Processing the request and the handlers it checks for various events and states around getting the file and reading in its content. If an event/state happened the event handlers are passed on to the <code><a href="reference/request_handler_handler.html">request_handler_handler()</a></code> along for problem resolution and collecting robots.txt file transformations:</p>
<ul>
<li>rule priorities decide if rules are applied given the current state priority</li>
<li>if rules specify signals those are emitted (e.g. error, message, warning)</li>
<li>often rules imply overwriting the raw content with a suitable interpretation given the circumstances the file was (or was not) retrieved</li>
</ul>
<p>Event handler rules can either consist of 4 items or can be functions - the former being the usual case and that used throughout the package itself. Functions like <code><a href="reference/paths_allowed.html">paths_allowed()</a></code> do have parameters that allow passing along handler rules or handler functions.</p>
<p>Handler rules are lists with the following items:</p>
<ul>
<li>
<code>over_write_file_with</code>: if the rule is triggered and has higher priority than those rules applied beforehand (i.e. the new priority has an higher value than the old priority) than the robots.txt file retrieved will be overwritten by this character vector</li>
<li>
<code>signal</code>: might be <code>"message"</code>, <code>"warning"</code>, or <code>"error"</code> and will use the signal function to signal the event/state just handled. Signaling a warning or a message might be suppressed by setting the function paramter <code>warn = FALSE</code>.</li>
<li>
<code>cache</code> should the package be allowed to cache the results of the retrieval or not</li>
<li>
<code>priority</code> the priority of the rule specified as numeric value, rules with higher priority will be allowed to overwrite robots.txt file content changed by rules with lower priority</li>
</ul>
<p>The package knows the following rules with the following defaults:</p>
<ul>
<li>
<code>on_server_error</code> :</li>
<li>given a server error - the server is unable to serve a file - we assume that something is terrible wrong and forbid all paths for the time being but do not cache the result so that we might get an updated file later on</li>
</ul>
<!-- end list --><div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">on_server_error_default</span></span>
<span><span class="co">## $over_write_file_with</span></span>
<span><span class="co">## [1] "User-agent: *\nDisallow: /"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $signal</span></span>
<span><span class="co">## [1] "error"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $cache</span></span>
<span><span class="co">## [1] FALSE</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $priority</span></span>
<span><span class="co">## [1] 20</span></span></code></pre></div>
<ul>
<li>
<code>on_client_error</code> :</li>
<li>client errors encompass all HTTP status 4xx status codes except 404 which is handled directly</li>
<li>despite the fact that there are a lot of codes that might indicate that the client has to take action (authentication, billing, … see: <a href="https://de.wikipedia.org/wiki/HTTP-Statuscode" class="external-link uri">https://de.wikipedia.org/wiki/HTTP-Statuscode</a>) in the case of retrieving robots.txt with simple GET request things should just work and any client error is treated as if there is no file available and thus scraping is generally allowed</li>
</ul>
<!-- end list --><div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">on_client_error_default</span></span>
<span><span class="co">## $over_write_file_with</span></span>
<span><span class="co">## [1] "User-agent: *\nAllow: /"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $signal</span></span>
<span><span class="co">## [1] "warning"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $cache</span></span>
<span><span class="co">## [1] TRUE</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $priority</span></span>
<span><span class="co">## [1] 19</span></span></code></pre></div>
<ul>
<li>
<code>on_not_found</code> :</li>
<li>HTTP status code 404 has its own handler but is treated the same ways other client errors: if there is no file available and thus scraping is generally allowed</li>
</ul>
<!-- end list --><div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">on_not_found_default</span></span>
<span><span class="co">## $over_write_file_with</span></span>
<span><span class="co">## [1] "User-agent: *\nAllow: /"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $signal</span></span>
<span><span class="co">## [1] "warning"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $cache</span></span>
<span><span class="co">## [1] TRUE</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $priority</span></span>
<span><span class="co">## [1] 1</span></span></code></pre></div>
<ul>
<li>
<code>on_redirect</code> :</li>
<li>redirects are ok - often redirects redirect from HTTP schema to HTTPS - robotstxt will use whatever content it has been redirected to</li>
</ul>
<!-- end list --><div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">on_redirect_default</span></span>
<span><span class="co">## $cache</span></span>
<span><span class="co">## [1] TRUE</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $priority</span></span>
<span><span class="co">## [1] 3</span></span></code></pre></div>
<ul>
<li>
<code>on_domain_change</code> :</li>
<li>domain changes are handled as if the robots.txt file did not exist and thus scraping is generally allowed</li>
</ul>
<!-- end list --><div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">on_domain_change_default</span></span>
<span><span class="co">## $signal</span></span>
<span><span class="co">## [1] "warning"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $cache</span></span>
<span><span class="co">## [1] TRUE</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $priority</span></span>
<span><span class="co">## [1] 4</span></span></code></pre></div>
<ul>
<li>
<code>on_file_type_mismatch</code> :</li>
<li>if {robotstxt} gets content with content type other than text it probably is not a robotstxt file, this situation is handled as if no file was provided and thus scraping is generally allowed</li>
</ul>
<!-- end list --><div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">on_file_type_mismatch_default</span></span>
<span><span class="co">## $over_write_file_with</span></span>
<span><span class="co">## [1] "User-agent: *\nAllow: /"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $signal</span></span>
<span><span class="co">## [1] "warning"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $cache</span></span>
<span><span class="co">## [1] TRUE</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $priority</span></span>
<span><span class="co">## [1] 6</span></span></code></pre></div>
<ul>
<li>
<code>on_suspect_content</code> :</li>
<li>if {robotstxt} cannot parse it probably is not a robotstxt file, this situation is handled as if no file was provided and thus scraping is generally allowed</li>
</ul>
<!-- end list --><div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">on_suspect_content_default</span></span>
<span><span class="co">## $over_write_file_with</span></span>
<span><span class="co">## [1] "User-agent: *\nAllow: /"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $signal</span></span>
<span><span class="co">## [1] "warning"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $cache</span></span>
<span><span class="co">## [1] TRUE</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $priority</span></span>
<span><span class="co">## [1] 7</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="design-map-for-eventstate-handling">Design Map for Event/State Handling<a class="anchor" aria-label="anchor" href="#design-map-for-eventstate-handling"></a>
</h3>
<p><strong>from version 0.7.x onwards</strong></p>
<p>While previous releases were concerned with implementing parsing and permission checking and improving performance the 0.7.x release will be about robots.txt retrieval foremost. While retrieval was implemented there are corner cases in the retrieval stage that very well influence the interpretation of permissions granted.</p>
<p><strong>Features and Problems handled:</strong></p>
<ul>
<li>now handles corner cases of retrieving robots.txt files</li>
<li>e.g. if no robots.txt file is available this basically means “you can scrape it all”</li>
<li>but there are further corner cases (what if there is a server error, what if redirection takes place, what is redirection takes place to different domains, what if a file is returned but it is not parsable, or is of format HTML or JSON, …)</li>
</ul>
<p><strong>Design Decisions</strong></p>
<ol style="list-style-type: decimal">
<li>the whole HTTP request-response-chain is checked for certain event/state types<ul>
<li>server error</li>
<li>client error</li>
<li>file not found (404)</li>
<li>redirection</li>
<li>redirection to another domain</li>
</ul>
</li>
<li>the content returned by the HTTP is checked against<ul>
<li>mime type / file type specification mismatch</li>
<li>suspicious content (file content does seem to be JSON, HTML, or XML instead of robots.txt)</li>
</ul>
</li>
<li>state/event handler define how these states and events are handled</li>
<li>a handler handler executes the rules defined in individual handlers</li>
<li>handler can be overwritten</li>
<li>handler defaults are defined that they should always do the right thing</li>
<li>handler can …<ul>
<li>overwrite the content of a robots.txt file (e.g. allow/disallow all)</li>
<li>modify how problems should be signaled: error, warning, message, none</li>
<li>if robots.txt file retrieval should be cached or not</li>
</ul>
</li>
<li>problems (no matter how they were handled) are attached to the robots.txt’s as attributes, allowing for …<ul>
<li>transparency</li>
<li>reacting post-mortem to the problems that occured</li>
</ul>
</li>
<li>all handler (even the actual execution of the HTTP-request) can be overwritten at runtime to inject user defined behaviour beforehand</li>
</ol>
</div>
<div class="section level3">
<h3 id="warnings">Warnings<a class="anchor" aria-label="anchor" href="#warnings"></a>
</h3>
<p>By default all functions retrieving robots.txt files will warn if there are</p>
<ul>
<li>any HTTP events happening while retrieving the file (e.g. redirects) or</li>
<li>the content of the file does not seem to be a valid robots.txt file.</li>
</ul>
<p>The warnings in the following example can be turned of in three ways:</p>
<p>(example)</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://docs.ropensci.org/robotstxt/">robotstxt</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="reference/paths_allowed.html">paths_allowed</a></span><span class="op">(</span><span class="st">"petermeissner.de"</span><span class="op">)</span></span>
<span><span class="co">##  petermeissner.de</span></span>
<span><span class="co">## [1] TRUE</span></span></code></pre></div>
<p>(solution 1)</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://docs.ropensci.org/robotstxt/">robotstxt</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/warning.html" class="external-link">suppressWarnings</a></span><span class="op">(</span><span class="op">{</span></span>
<span>  <span class="fu"><a href="reference/paths_allowed.html">paths_allowed</a></span><span class="op">(</span><span class="st">"petermeissner.de"</span><span class="op">)</span></span>
<span><span class="op">}</span><span class="op">)</span></span>
<span><span class="co">##  petermeissner.de</span></span>
<span><span class="co">## [1] TRUE</span></span></code></pre></div>
<p>(solution 2)</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://docs.ropensci.org/robotstxt/">robotstxt</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="reference/paths_allowed.html">paths_allowed</a></span><span class="op">(</span><span class="st">"petermeissner.de"</span>, warn <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="co">##  petermeissner.de</span></span>
<span><span class="co">## [1] TRUE</span></span></code></pre></div>
<p>(solution 3)</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://docs.ropensci.org/robotstxt/">robotstxt</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/options.html" class="external-link">options</a></span><span class="op">(</span>robotstxt_warn <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="reference/paths_allowed.html">paths_allowed</a></span><span class="op">(</span><span class="st">"petermeissner.de"</span><span class="op">)</span></span>
<span><span class="co">##  petermeissner.de</span></span>
<span><span class="co">## [1] TRUE</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="inspection-and-debugging">Inspection and Debugging<a class="anchor" aria-label="anchor" href="#inspection-and-debugging"></a>
</h3>
<p>The robots.txt files retrieved are basically mere character vectors:</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">rt</span> <span class="op">&lt;-</span> </span>
<span>  <span class="fu"><a href="reference/get_robotstxt.html">get_robotstxt</a></span><span class="op">(</span><span class="st">"petermeissner.de"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/character.html" class="external-link">as.character</a></span><span class="op">(</span><span class="va">rt</span><span class="op">)</span></span>
<span><span class="co">## [1] "# just do it - punk\n"</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="va">rt</span><span class="op">)</span></span>
<span><span class="co">## # just do it - punk</span></span></code></pre></div>
<p>The last HTTP request is stored in an object</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">rt_last_http</span><span class="op">$</span><span class="va">request</span></span>
<span><span class="co">## Response [https://petermeissner.de/robots.txt]</span></span>
<span><span class="co">##   Date: 2020-09-03 19:05</span></span>
<span><span class="co">##   Status: 200</span></span>
<span><span class="co">##   Content-Type: text/plain</span></span>
<span><span class="co">##   Size: 20 B</span></span>
<span><span class="co">## # just do it - punk</span></span></code></pre></div>
<p>But they also have some additional information stored as attributes.</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/names.html" class="external-link">names</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/attributes.html" class="external-link">attributes</a></span><span class="op">(</span><span class="va">rt</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">## [1] "problems" "cached"   "request"  "class"</span></span></code></pre></div>
<p>Events that might change the interpretation of the rules found in the robots.txt file:</p>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/attr.html" class="external-link">attr</a></span><span class="op">(</span><span class="va">rt</span>, <span class="st">"problems"</span><span class="op">)</span></span>
<span><span class="co">## $on_redirect</span></span>
<span><span class="co">## $on_redirect[[1]]</span></span>
<span><span class="co">## $on_redirect[[1]]$status</span></span>
<span><span class="co">## [1] 301</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $on_redirect[[1]]$location</span></span>
<span><span class="co">## [1] "https://petermeissner.de/robots.txt"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $on_redirect[[2]]</span></span>
<span><span class="co">## $on_redirect[[2]]$status</span></span>
<span><span class="co">## [1] 200</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $on_redirect[[2]]$location</span></span>
<span><span class="co">## NULL</span></span></code></pre></div>
<p>The {httr} request-response object that allwos to dig into what exactly was going on in the client-server exchange.</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/attr.html" class="external-link">attr</a></span><span class="op">(</span><span class="va">rt</span>, <span class="st">"request"</span><span class="op">)</span></span>
<span><span class="co">## Response [https://petermeissner.de/robots.txt]</span></span>
<span><span class="co">##   Date: 2020-09-03 19:05</span></span>
<span><span class="co">##   Status: 200</span></span>
<span><span class="co">##   Content-Type: text/plain</span></span>
<span><span class="co">##   Size: 20 B</span></span>
<span><span class="co">## # just do it - punk</span></span></code></pre></div>
<p>… or lets us retrieve the original content given back by the server:</p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">httr</span><span class="fu">::</span><span class="fu"><a href="https://httr.r-lib.org/reference/content.html" class="external-link">content</a></span><span class="op">(</span></span>
<span>  x        <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/attr.html" class="external-link">attr</a></span><span class="op">(</span><span class="va">rt</span>, <span class="st">"request"</span><span class="op">)</span>, </span>
<span>  as       <span class="op">=</span> <span class="st">"text"</span>,</span>
<span>  encoding <span class="op">=</span> <span class="st">"UTF-8"</span></span>
<span><span class="op">)</span></span>
<span><span class="co">## [1] "# just do it - punk\n"</span></span></code></pre></div>
<p>… or have a look at the actual HTTP request issued and all response headers given back by the server:</p>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># extract request-response object</span></span>
<span><span class="va">rt_req</span> <span class="op">&lt;-</span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/attr.html" class="external-link">attr</a></span><span class="op">(</span><span class="va">rt</span>, <span class="st">"request"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># HTTP request</span></span>
<span><span class="va">rt_req</span><span class="op">$</span><span class="va">request</span></span>
<span><span class="co">## &lt;request&gt;</span></span>
<span><span class="co">## GET http://petermeissner.de/robots.txt</span></span>
<span><span class="co">## Output: write_memory</span></span>
<span><span class="co">## Options:</span></span>
<span><span class="co">## * useragent: libcurl/7.64.1 r-curl/4.3 httr/1.4.1</span></span>
<span><span class="co">## * ssl_verifypeer: 1</span></span>
<span><span class="co">## * httpget: TRUE</span></span>
<span><span class="co">## Headers:</span></span>
<span><span class="co">## * Accept: application/json, text/xml, application/xml, */*</span></span>
<span><span class="co">## * user-agent: R version 3.6.3 (2020-02-29)</span></span>
<span></span>
<span><span class="co"># response headers</span></span>
<span><span class="va">rt_req</span><span class="op">$</span><span class="va">all_headers</span></span>
<span><span class="co">## [[1]]</span></span>
<span><span class="co">## [[1]]$status</span></span>
<span><span class="co">## [1] 301</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## [[1]]$version</span></span>
<span><span class="co">## [1] "HTTP/1.1"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## [[1]]$headers</span></span>
<span><span class="co">## $server</span></span>
<span><span class="co">## [1] "nginx/1.10.3 (Ubuntu)"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $date</span></span>
<span><span class="co">## [1] "Thu, 03 Sep 2020 19:05:45 GMT"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $`content-type`</span></span>
<span><span class="co">## [1] "text/html"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $`content-length`</span></span>
<span><span class="co">## [1] "194"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $connection</span></span>
<span><span class="co">## [1] "keep-alive"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $location</span></span>
<span><span class="co">## [1] "https://petermeissner.de/robots.txt"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## attr(,"class")</span></span>
<span><span class="co">## [1] "insensitive" "list"       </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## [[2]]</span></span>
<span><span class="co">## [[2]]$status</span></span>
<span><span class="co">## [1] 200</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## [[2]]$version</span></span>
<span><span class="co">## [1] "HTTP/1.1"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## [[2]]$headers</span></span>
<span><span class="co">## $server</span></span>
<span><span class="co">## [1] "nginx/1.10.3 (Ubuntu)"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $date</span></span>
<span><span class="co">## [1] "Thu, 03 Sep 2020 19:05:45 GMT"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $`content-type`</span></span>
<span><span class="co">## [1] "text/plain"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $`content-length`</span></span>
<span><span class="co">## [1] "20"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $`last-modified`</span></span>
<span><span class="co">## [1] "Thu, 03 Sep 2020 15:33:01 GMT"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $connection</span></span>
<span><span class="co">## [1] "keep-alive"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $etag</span></span>
<span><span class="co">## [1] "\"5f510cad-14\""</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $`accept-ranges`</span></span>
<span><span class="co">## [1] "bytes"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## attr(,"class")</span></span>
<span><span class="co">## [1] "insensitive" "list"</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="transformation">Transformation<a class="anchor" aria-label="anchor" href="#transformation"></a>
</h3>
<p>For convenience the package also includes a <code><a href="https://rdrr.io/r/base/list.html" class="external-link">as.list()</a></code> method for robots.txt files.</p>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">as.list</a></span><span class="op">(</span><span class="va">rt</span><span class="op">)</span></span>
<span><span class="co">## $content</span></span>
<span><span class="co">## [1] "# just do it - punk\n"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $robotstxt</span></span>
<span><span class="co">## [1] "# just do it - punk\n"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $problems</span></span>
<span><span class="co">## $problems$on_redirect</span></span>
<span><span class="co">## $problems$on_redirect[[1]]</span></span>
<span><span class="co">## $problems$on_redirect[[1]]$status</span></span>
<span><span class="co">## [1] 301</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $problems$on_redirect[[1]]$location</span></span>
<span><span class="co">## [1] "https://petermeissner.de/robots.txt"</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $problems$on_redirect[[2]]</span></span>
<span><span class="co">## $problems$on_redirect[[2]]$status</span></span>
<span><span class="co">## [1] 200</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $problems$on_redirect[[2]]$location</span></span>
<span><span class="co">## NULL</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $request</span></span>
<span><span class="co">## Response [https://petermeissner.de/robots.txt]</span></span>
<span><span class="co">##   Date: 2020-09-03 19:05</span></span>
<span><span class="co">##   Status: 200</span></span>
<span><span class="co">##   Content-Type: text/plain</span></span>
<span><span class="co">##   Size: 20 B</span></span>
<span><span class="co">## # just do it - punk</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="caching">Caching<a class="anchor" aria-label="anchor" href="#caching"></a>
</h3>
<p>The retrieval of robots.txt files is cached on a per R-session basis. Restarting an R-session will invalidate the cache. Also using the the function parameter <code>froce = TRUE</code> will force the package to re-retrieve the robots.txt file.</p>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="reference/paths_allowed.html">paths_allowed</a></span><span class="op">(</span><span class="st">"petermeissner.de/I_want_to_scrape_this_now"</span>, force <span class="op">=</span> <span class="cn">TRUE</span>, verbose <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co">##  petermeissner.de                      rt_robotstxt_http_getter: force http get</span></span>
<span><span class="co">## [1] TRUE</span></span>
<span><span class="fu"><a href="reference/paths_allowed.html">paths_allowed</a></span><span class="op">(</span><span class="st">"petermeissner.de/I_want_to_scrape_this_now"</span>,verbose <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co">##  petermeissner.de                      rt_robotstxt_http_getter: cached http get</span></span>
<span><span class="co">## [1] TRUE</span></span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="more-information">More information<a class="anchor" aria-label="anchor" href="#more-information"></a>
</h2>
<ul>
<li><a href="https://www.robotstxt.org/norobots-rfc.txt" class="external-link uri">https://www.robotstxt.org/norobots-rfc.txt</a></li>
<li><a href="https://cran.r-project.org/package=robotstxt/vignettes/using_robotstxt.html" class="external-link">Have a look at the vignette at https://cran.r-project.org/package=robotstxt/vignettes/using_robotstxt.html</a></li>
<li><a href="https://developers.google.com/search/reference/robots_txt?hl=en" class="external-link">Google on robots.txt</a></li>
<li><a href="https://wiki.selfhtml.org/wiki/Grundlagen/Robots.txt" class="external-link uri">https://wiki.selfhtml.org/wiki/Grundlagen/Robots.txt</a></li>
<li><a href="https://support.google.com/webmasters/answer/6062608?hl=en" class="external-link uri">https://support.google.com/webmasters/answer/6062608?hl=en</a></li>
<li><a href="https://www.robotstxt.org/robotstxt.html" class="external-link uri">https://www.robotstxt.org/robotstxt.html</a></li>
</ul>
</div>

  </main><aside class="col-md-3"><div class="links">
<h2 data-toc-skip>Links</h2>
<ul class="list-unstyled">
<li><a href="https://cloud.r-project.org/package=robotstxt" class="external-link">View on CRAN</a></li>
<li><a href="https://github.com/ropensci/robotstxt/" class="external-link">Browse source code</a></li>
<li><a href="https://github.com/ropensci/robotstxt/issues" class="external-link">Report a bug</a></li>
</ul>
</div>



<div class="license">
<h2 data-toc-skip>License</h2>
<ul class="list-unstyled">
<li>
<a href="https://opensource.org/licenses/mit-license.php" class="external-link">MIT</a> + file <a href="LICENSE-text.html">LICENSE</a>
</li>
</ul>
</div>


<div class="citation">
<h2 data-toc-skip>Citation</h2>
<ul class="list-unstyled">
<li><a href="authors.html#citation">Citing robotstxt</a></li>
</ul>
</div>

<div class="developers">
<h2 data-toc-skip>Developers</h2>
<ul class="list-unstyled">
<li>Pedro Baltazar <br><small class="roles"> Author, maintainer </small>  </li>
<li>Peter Meissner <br><small class="roles"> Author </small>  </li>
<li>Kun Ren <br><small class="roles"> Author, copyright holder </small>  </li>
<li><a href="authors.html">More about authors...</a></li>
</ul>
</div>

<div class="r-universe">
<h2 data-toc-skip>R-universe</h2>
<ul class="list-unstyled">
<li><p><a href="https://ropensci.r-universe.dev/robotstxt" class="external-link"><img src="https://ropensci.r-universe.dev/badges/robotstxt" alt="robotstxt status badge"></a></p></li>
</ul>
</div>

<div class="software-peer-review">
<h2 data-toc-skip>Software Peer-Review</h2>
<ul class="list-unstyled">
<li><p><a href="https://github.com/ropensci/software-review/issues/25" class="external-link"><img src="https://badges.ropensci.org/25_status.svg" alt="rOpenSci peer-review"></a></p></li>
</ul>
</div>

  </aside>
</div>


    <footer><!-- begin footer --><div class="footer">
    <div class="container">
        <div class="row start top-4 bottom-8">
            <div class="col-2"> <img id="footerlogo" src="https://ropensci.org/img/icon_short_white.svg">
</div>
            <div class="col-10">
                <div class="row">
                    <div class="col-md-4 col-xs-6">
                        <a href="https://github.com/ropensci" target="_blank" class="external-link"><div class="icon fab fa-github"></div></a>
                        <a href="https://github.com/ropenscilabs" target="_blank" class="external-link"><div class="icon fa fa-flask"></div></a>
                        <a href="https://hachyderm.io/@ropensci" target="_blank" class="external-link"><div class="icon fab fa-mastodon"></div></a>
                        <a href="https://vimeo.com/ropensci" target="_blank" class="external-link"><div class="icon fab fa-vimeo"></div></a>
                    </div>
                </div>
                <div class="row top-4">
                    <div class="col-md-2 col-sm-4">
                        <ul>
<h5 class="bottom-2">About</h5>
                            <li><a href="https://ropensci.org/about" class="external-link">About rOpenSci</a></li>
                            <li><a href="https://ropensci.org/software-review" class="external-link">Software Review</a></li>
                            <li><a href="https://ropensci.org/about#team" class="external-link">Our Team</a></li>
                            <li><a href="https://ropensci.org/careers" class="external-link">Jobs</a></li>
                            <li><a href="https://ropensci.org/donate" class="external-link">Donate</a></li>
                            <li><a href="https://ropensci.org/contact" class="external-link">Contact Us</a></li>
                        </ul>
</div>
                    <div class="col-md-3 col-sm-4">
                        <ul>
<h5 class="bottom-2">Community</h5>
                            <li><a href="https://ropensci.org/community/" class="external-link">Our Community</a></li>
                            <li><a href="https://ropensci.org/commcalls/" class="external-link">Community calls</a></li>
                            <li><a href="https://ropensci.org/events/" class="external-link">Events</a></li>
                            <li><a href="https://discuss.ropensci.org/" class="external-link">Join the Discussion</a></li>
                            <li><a href="https://ropensci.org/code-of-conduct" class="external-link">Code of conduct</a></li>
                        </ul>
</div>
                    <div class="col-md-2 col-sm-4">
                        <ul>
<h5 class="bottom-2">Resources</h5>
                            <li><a href="https://ropensci.org/packages/" class="external-link">Packages</a></li>
                            <li><a href="https://ropensci.org/usecases/" class="external-link">Use Cases</a></li>
                            <li><a href="https://ropensci.org/talks-papers/" class="external-link">Talks &amp; Publications</a></li>
                            <li><a href="https://docs.ropensci.org/" class="external-link">Documentation</a></li>
                            <li><a href="https://ropensci.org/news/" class="external-link">Newsletter</a></li>
                            <li><a href="https://ropensci.org/how-to-cite-ropensci/" class="external-link">Cite rOpenSci</a></li>
                        </ul>
</div>
                    <div class="col-md-4 col-xs-12">
                        <h5 class="bottom-2"></h5>

                        <p>rOpenSci is a fiscally sponsored project of <a href="http://numfocus.org" class="external-link">NumFOCUS</a>.</p>

                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
<!-- / end footer -->


    </footer>
</div>

  

  

  </body>
</html>
